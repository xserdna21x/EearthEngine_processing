{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c339e4",
   "metadata": {},
   "source": [
    "# 0. Installing geemap\n",
    "\n",
    "For this work is necesary install geemap library. To do it, follow the next page:\n",
    "\n",
    "* https://geemap.org/installation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9632a95c",
   "metadata": {},
   "source": [
    "# 1. Filtring by target contries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee98e3-774a-4341-bccf-105742ca8482",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b66e73-7e65-4c15-be42-b3e388625627",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "# import fiona\n",
    "import ee\n",
    "import geemap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger the authentication flow\n",
    "ee.Authenticate()\n",
    "\n",
    "# Initialize the library\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target countries: Option 1\n",
    "listCountries = ['Argentina', 'Bolivia', 'Brazil', 'Colombia', 'Ecuador',\n",
    "                 'Guatemala', 'Honduras', 'Mexico', 'Peru', 'Venezuela']\n",
    "\n",
    "# Load dataset\n",
    "dataset = ee.FeatureCollection(\"FAO/GAUL/2015/level2\")\n",
    "\n",
    "\n",
    "# Function to filter by country\n",
    "def filterCountry(country):\n",
    "    countries = ee.FeatureCollection(\"FAO/GAUL/2015/level2\")\n",
    "    filter_ = countries.filter(ee.Filter.eq('ADM0_NAME', country))\n",
    "    return filter_\n",
    "\n",
    "\n",
    "# create a list using the previous function to filter the nine countries\n",
    "list_filterCountries = [filterCountry(i) for i in listCountries]\n",
    "\n",
    "# Join the nine countries\n",
    "combinedCountries = list_filterCountries[0].merge(\n",
    "    list_filterCountries[1]).merge(\n",
    "    list_filterCountries[2]).merge(\n",
    "    list_filterCountries[3]).merge(\n",
    "    list_filterCountries[4]).merge(\n",
    "    list_filterCountries[5]).merge(\n",
    "    list_filterCountries[6]).merge(\n",
    "    list_filterCountries[7]).merge(\n",
    "    list_filterCountries[8]).merge(\n",
    "    list_filterCountries[9])\n",
    "\n",
    "# visualize the map\n",
    "Map = geemap.Map()\n",
    "tyleParams = {\n",
    "    'fillColor': 'b5ffb4',\n",
    "    'color': '00909F',\n",
    "    'width': 1.0\n",
    "}\n",
    "\n",
    "Map.addLayer(combinedCountries, tyleParams, 'Countries', 1)\n",
    "\n",
    "Map.addLayerControl()\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6233df9-3d43-4f86-8a7d-70f5dc37a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter target countries: Option 2\n",
    "listCountries = ['Argentina', 'Bolivia', 'Brazil', 'Colombia', 'Ecuador',\n",
    "                 'Guatemala', 'Honduras', 'Mexico', 'Peru', 'Venezuela']\n",
    "\n",
    "# Load dataset\n",
    "dataset = ee.FeatureCollection(\"FAO/GAUL/2015/level2\")\n",
    "\n",
    "targetCountries = dataset.filter(ee.Filter.inList(\n",
    "  'ADM0_NAME', listCountries))\n",
    "\n",
    "# Visualize map\n",
    "Map = geemap.Map()\n",
    "tyleParams = {\n",
    "    'fillColor': 'b5ffb4',\n",
    "    'color': '00909F',\n",
    "    'width': 1.0\n",
    "}\n",
    "\n",
    "Map.addLayer(targetCountries, tyleParams, 'Countries', 1)\n",
    "\n",
    "Map.addLayerControl()\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bfc064-353c-41aa-90e0-a92d45af0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate area in square kilometers\n",
    "def areaCountries(feature):\n",
    "    area = feature.geometry().area().divide(1e6)\n",
    "    return feature.set('areaSqKm', area)\n",
    "\n",
    "\n",
    "result2 = targetCountries.map(areaCountries)\n",
    "\n",
    "Map.addLayer(result2, tyleParams, 'TargetCountries', 1)\n",
    "\n",
    "# Map.addLayerControl()\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd3d76f",
   "metadata": {},
   "source": [
    "# 2. Global Forest Change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df665c0f",
   "metadata": {},
   "source": [
    "## 2.1 Quantifying Forest Change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef7bbf0",
   "metadata": {},
   "source": [
    "### 2.1.1. Select bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5888e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Global Forest Change dataset\n",
    "gfc2020 = ee.Image(\"UMD/hansen/global_forest_change_2020_v1_8\")\n",
    "\n",
    "# selecting bands\n",
    "treecover = gfc2020.select(['treecover2000'])\n",
    "lossImage = gfc2020.select(['loss'])\n",
    "lossYearImage = gfc2020.select(['lossyear'])\n",
    "gainImange = gfc2020.select(['gain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819010d4-5774-4c6d-bdab-0f0740747701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pixel size\n",
    "scale = lossImage.projection().nominalScale()\n",
    "print('In square meters: ', scale.getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f43d667",
   "metadata": {},
   "source": [
    "### 2.1.1.  Defining forest to thresholds of greater than 30, 50 and 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e25ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treecover grater than 70%\n",
    "treecover70 = treecover.gte(70).updateMask(treecover.gte(70))\n",
    "\n",
    "# treecover grather than 50%\n",
    "treecover50 = treecover.gte(50).updateMask(treecover.gte(50))\n",
    "\n",
    "# treecover grather than 30%\n",
    "treecover30 = treecover.gte(30).updateMask(treecover.gte(30))\n",
    "\n",
    "# Add tree covers\n",
    "Map.addLayer(treecover70, {\n",
    "    'palette': ['00FF00'],\n",
    "}, 'Forest_70')\n",
    "\n",
    "Map.addLayer(treecover50, {\n",
    "    'palette': ['00FF00']\n",
    "}, 'Forest_50')\n",
    "\n",
    "Map.addLayer(treecover30, {\n",
    "    'palette': ['00FF00']\n",
    "}, 'Forest_30')\n",
    "\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa43633",
   "metadata": {},
   "source": [
    "# 3. Zonal Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875be751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definiting temporal scale\n",
    "consecutive_numbers = np.arange(1, 21)\n",
    "years = np.arange(2001, 2021)\n",
    "\n",
    "consecutive_numbers, years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lossyear = [\n",
    "    lossYearImage.eq(i+1).mask(lossYearImage.eq(i+1)) for i in range(\n",
    "        len(consecutive_numbers))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe16d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list with names to download the zonal statistic\n",
    "lossyear70 = ['lossyear70_' + str(i) for i in years]\n",
    "\n",
    "lossyear50 = ['lossyear50_' + str(i) for i in years]\n",
    "\n",
    "lossyear30 = ['lossyear30_' + str(i) for i in years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396206f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use And() method to create loss band by year according to the porcentage of\n",
    "\n",
    "# treecover previously defined\n",
    "\n",
    "deforestation50 = [i.And(treecover50) for i in list_lossyear]\n",
    "deforestation70 = [i.And(treecover70) for i in list_lossyear]\n",
    "deforestation30 = [i.And(treecover30) for i in list_lossyear]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093089c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add layers\n",
    "Map = geemap.Map(center=(4, -75), zoom=8)\n",
    "\n",
    "Map.addLayer(list_lossyear[9], {'palette': ['000000']}, 'lossYear')\n",
    "Map.addLayer(deforestation50[9], {'palette': ['00FF00']}, 'deforestation_50')\n",
    "Map.addLayer(deforestation70[9], {'palette': ['FF0000']}, 'deforestation_70')\n",
    "Map.addLayer(deforestation30[0], {'palette': ['FF0000']}, 'deforestation_30')\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148f5352",
   "metadata": {},
   "source": [
    "## 3.1. For each loss year using treecover30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfdc774-2182-49b2-a625-02e7d01d6220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TargetCountries30(country):\n",
    "    # filter for target countries\n",
    "    countryTarget = result2.filter(ee.Filter.eq(\n",
    "      'ADM0_NAME', country))\n",
    "\n",
    "    for i in range(len(consecutive_numbers)):\n",
    "        out_dir = os.path.join('outputs_zonalStatistic')\n",
    "        out_countries_stats = os.path.join(\n",
    "            out_dir, lossyear30[i] + country + '.csv')\n",
    "\n",
    "        geemap.zonal_statistics(\n",
    "            deforestation30[i],\n",
    "            countryTarget,\n",
    "            out_countries_stats,\n",
    "            statistics_type='SUM',\n",
    "            scale=30\n",
    "        )\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Load csv layers generated in the previous step\n",
    "\n",
    "    filename = r'outputs_zonalStatistic/'\n",
    "\n",
    "    filenames = ['lossyear30_' + str(f) + country + '.csv' for f in years]\n",
    "\n",
    "    df_DEF_country30 = [pd.read_csv(filename + j) for j in filenames]\n",
    "\n",
    "    # Calculate area in ha from result of the zonal statistic\n",
    "    for i in range(len(df_DEF_country30)):\n",
    "        df_DEF_country30[i]['loss30_ha_'+str(years[i])] = (\n",
    "            df_DEF_country30[i]['sum']*(scale**2))/10000\n",
    "\n",
    "    # download for only one time the FeatureCollection of the countries\n",
    "    # in shape format\n",
    "\n",
    "    # This is to get a base in shp format to join csv's fiels.\n",
    "    out_dir = os.path.join('outputs_zonalStatistic/countriesSHP')\n",
    "    out_countries_stats = os.path.join(out_dir, country+'30.shp')\n",
    "\n",
    "    geemap.zonal_statistics(\n",
    "        deforestation30[0],\n",
    "        countryTarget,\n",
    "        out_countries_stats,\n",
    "        statistics_type='SUM',\n",
    "        scale=scale)\n",
    "\n",
    "    # Drop unnecesary columns\n",
    "    df_DEF_country30_drop = []\n",
    "    for i in df_DEF_country30:\n",
    "        df_DEF_country30_drop.append(i.drop([\n",
    "            'sum', 'system:index', 'ADM1_CODE', 'EXP2_YEAR', 'ADM2_NAME',\n",
    "            'ADM1_NAME', 'DISP_AREA', 'Shape_Leng', 'STATUS', 'STR2_YEAR',\n",
    "            'ADM0_NAME', 'ADM0_CODE', 'Shape_Area', 'areaSqKm'\n",
    "        ], axis=1))\n",
    "\n",
    "    gpd_country_30 = gpd.read_file(\n",
    "        r'outputs_zonalStatistic/countriesSHP/'+country+'30.shp')\n",
    "    gpd_country_30 = gpd_country_30.drop(['sum'], axis=1)\n",
    "\n",
    "    gpd_country_30 = gpd_country_30.set_index('ADM2_CODE')\n",
    "\n",
    "    for i in df_DEF_country30_drop:\n",
    "        gpd_country_30 = pd.concat([gpd_country_30, i.set_index('ADM2_CODE')],\n",
    "                                   axis=1)\n",
    "\n",
    "    out_dir = os.path.join('outputs_zonalStatistic')\n",
    "    out_countries_stats = os.path.join(out_dir,\n",
    "                                       'treecover2000_30_'+country+'.csv')\n",
    "\n",
    "    geemap.zonal_statistics(\n",
    "        treecover30,\n",
    "        countryTarget,\n",
    "        out_countries_stats,\n",
    "        statistics_type='SUM',\n",
    "        scale=scale)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # concat treecover 30%\n",
    "    df_treecover2000_country30 = pd.read_csv(\n",
    "        r'outputs_zonalStatistic/treecover2000_30_'+country+'.csv')\n",
    "\n",
    "    df_treecover2000_country30['forest_30'] = (\n",
    "        df_treecover2000_country30['sum']*900)/10000\n",
    "\n",
    "    df_treecover2000_country30 = df_treecover2000_country30.drop(\n",
    "        ['sum', 'system:index', 'ADM1_CODE', 'ADM2_NAME', 'EXP2_YEAR',\n",
    "         'ADM1_NAME', 'DISP_AREA', 'Shape_Leng', 'STATUS', 'STR2_YEAR',\n",
    "         'ADM0_NAME', 'ADM0_CODE', 'Shape_Area'], axis=1)\n",
    "\n",
    "    gpd_country_30 = pd.concat([\n",
    "        gpd_country_30, df_treecover2000_country30.set_index('ADM2_CODE')],\n",
    "        axis=1)\n",
    "\n",
    "    gpd_country_30 = gpd_country_30.drop([\n",
    "        'loss30_ha_2001', 'loss30_ha_2002',\n",
    "        'loss30_ha_2003', 'loss30_ha_2004',\n",
    "        'loss30_ha_2005', 'loss30_ha_2006',\n",
    "        'loss30_ha_2007', 'loss30_ha_2008'\n",
    "    ], axis=1)\n",
    "\n",
    "    gpd_country_30.to_file(\n",
    "        'gpkg/V1/thresholds.gpkg', layer=country+str(30), driver=\"GPKG\")\n",
    "\n",
    "    # return gpd_country_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550474f5-0f23-46ee-8d7e-0947aa4a27e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "listC = ['Argentina', 'Bolivia', 'Colombia', 'Ecuador', 'Guatemala', \n",
    "         'Honduras', 'Mexico', 'Peru', 'Venezuela']\n",
    "\n",
    "for i in listC:\n",
    "    TargetCountries30(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ed23c-eec5-402c-a277-90556a81657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to split the state of Brazil into parts to reduce computational process.\n",
    "lista1 = [i for i in range(6333, 8171)]\n",
    "lista2 = [i for i in range(8171, 10008)]\n",
    "lista3 = [i for i in range(10008, 11843)]\n",
    "\n",
    "print(lista1[-1], lista2[-1], lista3[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59472e54-f82d-44ac-bec9-d5d79e55f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by target countries\n",
    "Brazil = result2.filter(ee.Filter.eq(\n",
    "  'ADM0_NAME', 'Brazil'));\n",
    "\n",
    "StatesB1 = Brazil.filter(ee.Filter.inList('ADM2_CODE', lista1))\n",
    "StatesB2 = Brazil.filter(ee.Filter.inList('ADM2_CODE', lista2))\n",
    "StatesB3 = Brazil.filter(ee.Filter.inList('ADM2_CODE', lista3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e07bf8-45d9-41e0-b392-455db1788a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Map.addLayer(StatesB2, tyleParams, 'TargetCountries', 1)\n",
    "#Map.addLayerControl()\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d863a8e-92af-48d3-8117-545714a7a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code piece generate one csv file by each loss year and country\n",
    "country = StatesB1\n",
    "for i in range(len(consecutive_numbers)):\n",
    "    out_dir = os.path.join('outputs_zonalStatistic')\n",
    "    out_countries_stats = os.path.join(\n",
    "        out_dir, lossyear30[i] + 'StatesB1'+'.csv')\n",
    "    \n",
    "    geemap.zonal_statistics(\n",
    "        deforestation30[i],\n",
    "        country,\n",
    "        out_countries_stats,\n",
    "        statistics_type='SUM',\n",
    "        scale=scale.getInfo()\n",
    "    )\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load csv layers generated in the previous step\n",
    "\n",
    "filename = r'outputs_zonalStatistic/'\n",
    "#nn = np.arange(2001,2019,1)\n",
    "filenames = ['lossyear30_' + str(f) + 'StatesB1'+'.csv' for f in years]\n",
    "\n",
    "df_DEF_StatesB130 = [pd.read_csv(filename + j) for j in filenames]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Calculate area in ha from zonal statistic result\n",
    "scale = scale.getInfo()\n",
    "for i in range(len(df_DEF_StatesB130)):\n",
    "    df_DEF_StatesB130[i]['loss30_ha_' + str(years[i])] = (\n",
    "        (df_DEF_StatesB130[i]['sum']*(scale**2))/10000\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# download only once the FeatureCollection of the countries in shape format\n",
    "# Uncomment the next lines to download it ONLY ONCE.\n",
    "# This is to get a base in shp format to join csv's fiels.\n",
    "out_dir = os.path.join('outputs_zonalStatistic/countriesSHP')\n",
    "out_countries_stats = os.path.join(out_dir, 'StatesB1_50.shp')\n",
    "    \n",
    "geemap.zonal_statistics(\n",
    "    deforestation30[0], \n",
    "    country, \n",
    "    out_countries_stats, \n",
    "    statistics_type='SUM', \n",
    "    scale=scale)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Drop unnecesary columns\n",
    "df_DEF_StatesB130_drop = []\n",
    "for i in df_DEF_StatesB130:\n",
    "    df_DEF_StatesB130_drop.append(i.drop([\n",
    "        'sum', 'system:index', 'ADM1_CODE', 'EXP2_YEAR', 'ADM2_NAME', \n",
    "        'ADM1_NAME', 'DISP_AREA', 'Shape_Leng', 'STATUS', 'STR2_YEAR',\n",
    "        'ADM0_NAME', 'ADM0_CODE', 'Shape_Area', 'areaSqKm'\n",
    "    ], axis=1))\n",
    "    \n",
    "# -----------------------------------------------------------------------------   \n",
    "gpd_StatesB1_30 = gpd.read_file(\n",
    "    r'outputs_zonalStatistic/countriesSHP/StatesB1_30.shp')\n",
    "\n",
    "gpd_StatesB1_30 = gpd_StatesB1_30.drop(['sum'], axis=1)\n",
    "\n",
    "gpd_StatesB1_30 = gpd_StatesB1_30.set_index('ADM2_CODE')\n",
    "\n",
    "for i in df_DEF_StatesB130_drop:\n",
    "    gpd_StatesB1_30 = pd.concat([gpd_StatesB1_30, i.set_index('ADM2_CODE')],\n",
    "                                axis=1)\n",
    "# -----------------------------------------------------------------------------\n",
    "out_dir = os.path.join('outputs_zonalStatistic')\n",
    "out_countries_stats = os.path.join(out_dir,\n",
    "                                   'treecover2000_30_StatesB1.csv')\n",
    "    \n",
    "geemap.zonal_statistics(\n",
    "    treecover30,\n",
    "    country,\n",
    "    out_countries_stats, \n",
    "    statistics_type='SUM',\n",
    "    scale=scale)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# concat with treecover 30%\n",
    "df_treecover2000_StatesB130 = pd.read_csv(\n",
    "    r'outputs_zonalStatistic/treecover2000_30_StatesB1.csv')\n",
    "\n",
    "df_treecover2000_StatesB130['forest_30'] = (\n",
    "    (df_treecover2000_StatesB130['sum']*(scale**2))/10000)\n",
    "\n",
    "df_treecover2000_StatesB130 = df_treecover2000_StatesB130.drop(\n",
    "    ['sum', 'system:index', 'ADM1_CODE', 'ADM2_NAME', 'EXP2_YEAR',\n",
    "     'ADM1_NAME', 'DISP_AREA', 'Shape_Leng', 'STATUS', 'STR2_YEAR',\n",
    "     'ADM0_NAME', 'ADM0_CODE', 'Shape_Area','areaSqKm'], \n",
    "    axis=1)\n",
    "\n",
    "gpd_StatesB1_30 = pd.concat([\n",
    "    gpd_StatesB1_30, \n",
    "    df_treecover2000_StatesB130.set_index('ADM2_CODE')], axis=1)\n",
    "\n",
    "gpd_StatesB1_30 = gpd_StatesB1_30.drop(\n",
    "    ['loss30_ha_2001', 'loss30_ha_2002', \n",
    "    'loss30_ha_2003', 'loss30_ha_2004',\n",
    "    'loss30_ha_2005', 'loss30_ha_2006',\n",
    "    'loss30_ha_2007', 'loss30_ha_2008'], axis=1)\n",
    "\n",
    "\n",
    "gpd_StatesB1_30.to_file('gpkg/V1/thresholds.gpkg', layer='StatesB1_30',\n",
    "                        driver=\"GPKG\")\n",
    "\n",
    "gpd_StatesB1_30[['areaSqKm','forest_30']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4b23f-84bc-4984-8b06-500a85de01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for target countries\n",
    "#country = result2.filter(ee.Filter.eq(\n",
    "#    'ADM0_NAME', 'StatesB2'));\n",
    "country = StatesB2\n",
    "for i in range(len(consecutive_numbers)):\n",
    "    out_dir = os.path.join('outputs_zonalStatistic')\n",
    "    out_countries_stats = os.path.join(\n",
    "        out_dir, lossyear30[i] + 'StatesB2'+'.csv')\n",
    "    \n",
    "    geemap.zonal_statistics(\n",
    "        deforestation30[i],\n",
    "        country,\n",
    "        out_countries_stats,\n",
    "        statistics_type='SUM',\n",
    "        scale=scale\n",
    "    )\n",
    "# ----------------------------------------------------------------------------\n",
    "# Load csv layers generated in the previous step\n",
    "\n",
    "filename = r'outputs_zonalStatistic/'\n",
    "#nn = np.arange(2001,2019,1)\n",
    "filenames = ['lossyear30_' + str(f) + 'StatesB2'+'.csv' for f in years]\n",
    "\n",
    "df_DEF_StatesB230 = [pd.read_csv(filename + j) for j in filenames]\n",
    "\n",
    "# Calculate area in ha from result of the zonal statistic\n",
    "scale = scale.getInfo()\n",
    "for i in range(len(df_DEF_StatesB230)):\n",
    "    df_DEF_StatesB230[i]['loss30_ha_' + str(years[i])] = (\n",
    "        (df_DEF_StatesB230[i]['sum']*900)/10000\n",
    "    )\n",
    "    \n",
    "# download for only one time the FeatureCollection of the countries in \n",
    "# shape format\n",
    "\n",
    "# This is to get a base in shp format to join csv's fiels.\n",
    "out_dir = os.path.join('outputs_zonalStatistic/countriesSHP')\n",
    "out_countries_stats = os.path.join(out_dir, 'StatesB2_30.shp')\n",
    "    \n",
    "geemap.zonal_statistics(\n",
    "    deforestation30[0], \n",
    "    country,\n",
    "    out_countries_stats,\n",
    "    statistics_type='SUM',\n",
    "    scale=scale)\n",
    "    \n",
    "# Drop unnecesary columns\n",
    "df_DEF_StatesB230_drop = []\n",
    "for i in df_DEF_StatesB230:\n",
    "    df_DEF_StatesB230_drop.append(i.drop([\n",
    "        'sum', 'system:index', 'ADM1_CODE', 'EXP2_YEAR', 'ADM2_NAME', \n",
    "        'ADM1_NAME', 'DISP_AREA', 'Shape_Leng', 'STATUS', 'STR2_YEAR',\n",
    "        'ADM0_NAME', 'ADM0_CODE', 'Shape_Area', 'areaSqKm'\n",
    "    ], axis=1))\n",
    "    \n",
    "    \n",
    "gpd_StatesB2_30 = gpd.read_file(\n",
    "    r'outputs_zonalStatistic/countriesSHP/StatesB2_30.shp')\n",
    "gpd_StatesB2_30 = gpd_StatesB2_30.drop(['sum'], axis=1)\n",
    "\n",
    "gpd_StatesB2_30 = gpd_StatesB2_30.set_index('ADM2_CODE')\n",
    "\n",
    "for i in df_DEF_StatesB230_drop:\n",
    "    gpd_StatesB2_30 = pd.concat([gpd_StatesB2_30, i.set_index('ADM2_CODE')], \n",
    "                                axis=1)\n",
    "# -----------------------------------------------------------------------------\n",
    "out_dir = os.path.join('outputs_zonalStatistic')\n",
    "out_countries_stats = os.path.join(out_dir,\n",
    "                                   'treecover2000_30_StatesB2.csv')\n",
    "    \n",
    "geemap.zonal_statistics(\n",
    "    treecover30,\n",
    "    StatesB2,\n",
    "    out_countries_stats,\n",
    "    statistics_type='SUM',\n",
    "    scale=scale)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# concat with treecover 30%\n",
    "df_treecover2000_StatesB230 = pd.read_csv(\n",
    "    r'outputs_zonalStatistic/treecover2000_30_StatesB2.csv')\n",
    "\n",
    "df_treecover2000_StatesB230['forest_30'] = (\n",
    "    df_treecover2000_StatesB230['sum']*(scale**2))/10000\n",
    "\n",
    "df_treecover2000_StatesB230 = df_treecover2000_StatesB230.drop([\n",
    "    'sum', 'system:index', 'ADM1_CODE', 'ADM2_NAME', 'EXP2_YEAR',\n",
    "    'ADM1_NAME', 'DISP_AREA', 'Shape_Leng', 'STATUS', 'STR2_YEAR',\n",
    "    'ADM0_NAME', 'ADM0_CODE', 'Shape_Area'], axis=1)\n",
    "\n",
    "gpd_StatesB2_30 = pd.concat([\n",
    "    gpd_StatesB2_30, df_treecover2000_StatesB230.set_index('ADM2_CODE')],\n",
    "    axis=1)\n",
    "\n",
    "gpd_StatesB2_30 = gpd_StatesB2_30.drop([\n",
    "    'loss30_ha_2001', 'loss30_ha_2002', \n",
    "    'loss30_ha_2003', 'loss30_ha_2004',\n",
    "    'loss30_ha_2005', 'loss30_ha_2006',\n",
    "    'loss30_ha_2007', 'loss30_ha_2008'\n",
    "], axis=1)\n",
    "\n",
    "\n",
    "gpd_StatesB2_30.to_file('gpkg/V1/thresholds.gpkg', layer='StatesB2_30',\n",
    "                        driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c412e-0783-4e98-a30f-7475fce08ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "country = StatesB3\n",
    "for i in range(len(consecutive_numbers)):\n",
    "    out_dir = os.path.join('outputs_zonalStatistic')\n",
    "    out_countries_stats = os.path.join(\n",
    "        out_dir, lossyear30[i] + 'StatesB3'+'.csv')\n",
    "    \n",
    "    geemap.zonal_statistics(\n",
    "        deforestation30[i],\n",
    "        country,\n",
    "        out_countries_stats,\n",
    "        statistics_type='SUM',\n",
    "        scale=scale\n",
    "    )\n",
    "#-------------------------------------------------------------------\n",
    "# Load csv layers generated in the previous step\n",
    "\n",
    "filename = r'outputs_zonalStatistic/'\n",
    "#nn = np.arange(2001,2019,1)\n",
    "filenames = ['lossyear30_' + str(f) + 'StatesB3'+'.csv' for f in years]\n",
    "\n",
    "df_DEF_StatesB330 = [pd.read_csv(filename + j) for j in filenames]\n",
    "\n",
    "# Calculate area in ha from result of the zonal statistic\n",
    "scale = scale.getInfo()\n",
    "for i in range(len(df_DEF_StatesB330)):\n",
    "    df_DEF_StatesB330[i]['loss30_ha_' + str(years[i])] = (\n",
    "        (df_DEF_StatesB330[i]['sum']*(scale**2))/10000\n",
    "    )\n",
    "    \n",
    "# download for only one time the FeatureCollection of the countries in\n",
    "# shape format\n",
    "\n",
    "# This is to get a base in shp format to join csv's fiels.\n",
    "out_dir = os.path.join('outputs_zonalStatistic/countriesSHP')\n",
    "out_countries_stats = os.path.join(out_dir, 'StatesB3_50.shp')\n",
    "    \n",
    "geemap.zonal_statistics(\n",
    "    deforestation30[0],\n",
    "    country,\n",
    "    out_countries_stats, \n",
    "    statistics_type='SUM', \n",
    "    scale=scale)\n",
    "    \n",
    "# Drop unnecesary columns\n",
    "df_DEF_StatesB330_drop = []\n",
    "for i in df_DEF_StatesB330:\n",
    "    df_DEF_StatesB330_drop.append(i.drop([\n",
    "        'sum', 'system:index', 'ADM1_CODE', 'EXP2_YEAR', 'ADM2_NAME', \n",
    "        'ADM1_NAME', 'DISP_AREA', 'Shape_Leng', 'STATUS', 'STR2_YEAR',\n",
    "        'ADM0_NAME', 'ADM0_CODE', 'Shape_Area', 'areaSqKm'\n",
    "    ], axis=1))\n",
    "    \n",
    "    \n",
    "gpd_StatesB3_30 = gpd.read_file(\n",
    "    r'outputs_zonalStatistic/countriesSHP/StatesB3_30.shp')\n",
    "gpd_StatesB3_30 = gpd_StatesB3_30.drop(['sum'], axis=1)\n",
    "\n",
    "gpd_StatesB3_30 = gpd_StatesB3_30.set_index('ADM2_CODE')\n",
    "\n",
    "for i in df_DEF_StatesB330_drop:\n",
    "    gpd_StatesB3_30 = pd.concat([gpd_StatesB3_30, i.set_index('ADM2_CODE')],\n",
    "                                axis=1)\n",
    "    \n",
    "out_dir = os.path.join('outputs_zonalStatistic')\n",
    "out_countries_stats = os.path.join(out_dir,\n",
    "                                   'treecover2000_30_StatesB3.csv')\n",
    "    \n",
    "geemap.zonal_statistics(\n",
    "    treecover30,\n",
    "    country,\n",
    "    out_countries_stats,\n",
    "    statistics_type='SUM',\n",
    "    scale=scale)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# concat with treecover 30%\n",
    "df_treecover2000_StatesB330 = pd.read_csv(\n",
    "    r'outputs_zonalStatistic/treecover2000_30_StatesB3.csv')\n",
    "\n",
    "df_treecover2000_StatesB330['forest_30'] = (\n",
    "    (df_treecover2000_StatesB330['sum']*(scale**2))/10000)\n",
    "\n",
    "df_treecover2000_StatesB330 = df_treecover2000_StatesB330.drop(\n",
    "    ['sum', 'system:index', 'ADM1_CODE', 'ADM2_NAME', 'EXP2_YEAR',\n",
    "     'ADM1_NAME', 'DISP_AREA', 'Shape_Leng', 'STATUS', 'STR2_YEAR',\n",
    "     'ADM0_NAME', 'ADM0_CODE', 'Shape_Area','areaSqKm'], \n",
    "    axis=1)\n",
    "\n",
    "gpd_StatesB3_30 = pd.concat([\n",
    "    gpd_StatesB3_30,\n",
    "    df_treecover2000_StatesB330.set_index('ADM2_CODE')], axis=1)\n",
    "\n",
    "gpd_StatesB3_30 = gpd_StatesB3_30.drop(\n",
    "    ['loss30_ha_2001', 'loss30_ha_2002', \n",
    "    'loss30_ha_2003', 'loss30_ha_2004',\n",
    "    'loss30_ha_2005', 'loss30_ha_2006',\n",
    "    'loss30_ha_2007', 'loss30_ha_2008'], axis=1)\n",
    "\n",
    "\n",
    "gpd_StatesB3_30.to_file('gpkg/V1/thresholds.gpkg', layer='StatesB3_30',\n",
    "                        driver=\"GPKG\")\n",
    "\n",
    "gpd_StatesB3_30[['areaSqKm','forest_30']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f3cb6-cab6-421c-8b8d-42260921b02e",
   "metadata": {},
   "source": [
    "listCountries = ['Argentina', 'Bolivia', 'Brazil', 'Colombia', 'Ecuador',\n",
    "                 'Guatemala', 'Honduras', 'Mexico', 'Peru', 'Venezuela']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7099e2db-a4f8-494b-a903-1f7352a21263",
   "metadata": {},
   "source": [
    "## 3.2 For each loss year using treecover50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f098f24-2aba-45af-978a-982627c9f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = lossImage.projection().nominalScale()\n",
    "scale = scale.getInfo()\n",
    "    \n",
    "def TargetCountries50(country):\n",
    "        # filter for target countries\n",
    "    countryTarget = result2.filter(ee.Filter.eq(\n",
    "      'ADM0_NAME', country))\n",
    "\n",
    "    for i in range(len(consecutive_numbers)):\n",
    "        out_dir = os.path.join('outputs_zonalStatistic')\n",
    "        out_countries_stats = os.path.join(\n",
    "            out_dir, lossyear50[i] + country+'.csv')\n",
    "\n",
    "        geemap.zonal_statistics(\n",
    "            deforestation50[i],\n",
    "            countryTarget,\n",
    "            out_countries_stats,\n",
    "            statistics_type='SUM',\n",
    "            scale=scale\n",
    "        )\n",
    "    #-------------------------------------------------------------------\n",
    "    # Load csv layers generated in the previous step\n",
    "\n",
    "    filename = r'outputs_zonalStatistic/'\n",
    "    #nn = np.arange(2001,2019,1)\n",
    "    filenames = ['lossyear50_' + str(f) + country + '.csv' for f in years]\n",
    "\n",
    "    df_DEF_country50 = [pd.read_csv(filename + j) for j in filenames]\n",
    "\n",
    "    # Calculate area in ha from result of the zonal statistic\n",
    "    for i in range(len(df_DEF_country50)):\n",
    "        df_DEF_country50[i]['loss50_ha_'+str(years[i])] = (\n",
    "            df_DEF_country50[i]['sum']*(scale**2))/10000\n",
    "\n",
    "    # download for only one time the FeatureCollection of the countries in \n",
    "    # shape format\n",
    "\n",
    "    # This is to get a base in shp format to join csv's fiels.\n",
    "    out_dir = os.path.join('outputs_zonalStatistic/countriesSHP')\n",
    "    out_countries_stats = os.path.join(out_dir, country+'.shp')\n",
    "\n",
    "    geemap.zonal_statistics(\n",
    "        deforestation50[0], \n",
    "        countryTarget, \n",
    "        out_countries_stats, \n",
    "        statistics_type='SUM', \n",
    "        scale=scale)\n",
    "\n",
    "    # Drop unnecesary columns\n",
    "    df_DEF_country50_drop = []\n",
    "    for i in df_DEF_country50:\n",
    "        df_DEF_country50_drop.append(i.drop([\n",
    "            'sum', 'system:index', 'ADM1_CODE', 'EXP2_YEAR', 'ADM2_NAME', \n",
    "            'ADM1_NAME', 'DISP_AREA', 'Shape_Leng', 'STATUS', 'STR2_YEAR',\n",
    "            'ADM0_NAME', 'ADM0_CODE', 'Shape_Area', 'areaSqKm'\n",
    "        ], axis=1))\n",
    "\n",
    "\n",
    "    gpd_country_50 = gpd.read_file(\n",
    "        r'outputs_zonalStatistic/countriesSHP/'+country+'.shp')\n",
    "    gpd_country_50 = gpd_country_50.drop(['sum'], axis=1)\n",
    "\n",
    "    gpd_country_50 = gpd_country_50.set_index('ADM2_CODE')\n",
    "\n",
    "    for i in df_DEF_country50_drop:\n",
    "        gpd_country_50 = pd.concat([gpd_country_50, i.set_index('ADM2_CODE')],\n",
    "                                   axis=1)\n",
    "        \n",
    "        \n",
    "    out_dir = os.path.join('outputs_zonalStatistic')\n",
    "    out_countries_stats = os.path.join(out_dir,\n",
    "                                       'treecover2000_50_'+country+'.csv')\n",
    "\n",
    "    geemap.zonal_statistics(\n",
    "        treecover50, \n",
    "        countryTarget, \n",
    "        out_countries_stats, \n",
    "        statistics_type='SUM',\n",
    "        scale=scale)\n",
    "\n",
    "    #---------------------------------------------------------------------\n",
    "    # concat treecover 50%\n",
    "    df_treecover2000_country50 = pd.read_csv(\n",
    "        r'outputs_zonalStatistic/treecover2000_50_'+country+'.csv')\n",
    "\n",
    "    df_treecover2000_country50['forest_50'] = (\n",
    "        df_treecover2000_country50['sum']*(scale**2))/10000\n",
    "\n",
    "    df_treecover2000_country50 = df_treecover2000_country50.drop(\n",
    "        ['sum', 'system:index', 'ADM1_CODE', 'ADM2_NAME', 'EXP2_YEAR',\n",
    "         'ADM1_NAME', 'DISP_AREA', 'Shape_Leng', 'STATUS', 'STR2_YEAR', \n",
    "         'ADM0_NAME', 'ADM0_CODE', 'Shape_Area'], axis=1)\n",
    "\n",
    "    gpd_country_50 = pd.concat([\n",
    "        gpd_country_50, df_treecover2000_country50.set_index('ADM2_CODE')],\n",
    "        axis=1)\n",
    "\n",
    "    gpd_country_50 = gpd_country_50.drop([\n",
    "        'loss50_ha_2001', 'loss50_ha_2002', \n",
    "        'loss50_ha_2003', 'loss50_ha_2004',\n",
    "        'loss50_ha_2005', 'loss50_ha_2006',\n",
    "        'loss50_ha_2007', 'loss50_ha_2008'\n",
    "    ], axis=1)\n",
    "\n",
    "\n",
    "    gpd_country_50.to_file('gpkg/V1/thresholds.gpkg', layer=country,\n",
    "                           driver=\"GPKG\")\n",
    "    \n",
    "    return gpd_country_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf5c60-424b-4623-8e66-db6ce96cc7b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "listC = ['Bolivia', 'Colombia', 'Ecuador', 'Guatemala', 'Honduras', 'Peru',\n",
    "         'Venezuela']\n",
    "\n",
    "for i in listC:\n",
    "    TargetCountries50(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a03d01-77b0-4908-82ff-2313e2b1e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code piece generate one csv file by each loss year and country\n",
    "country = StatesB1\n",
    "for i in range(len(consecutive_numbers)):\n",
    "    out_dir = os.path.join('outputs_zonalStatistic')\n",
    "    out_countries_stats = os.path.join(\n",
    "        out_dir, lossyear50[i] + 'StatesB1'+'.csv')\n",
    "    \n",
    "    geemap.zonal_statistics(\n",
    "        deforestation50[i],\n",
    "        country,\n",
    "        out_countries_stats,\n",
    "        statistics_type='SUM',\n",
    "        scale=scale.getInfo()\n",
    "    )\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load csv layers generated in the previous step\n",
    "\n",
    "filename = r'outputs_zonalStatistic/'\n",
    "#nn = np.arange(2001,2019,1)\n",
    "filenames = ['lossyear50_' + str(f) + 'StatesB1'+'.csv' for f in years]\n",
    "\n",
    "df_DEF_StatesB150 = [pd.read_csv(filename + j) for j in filenames]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Calculate area in ha from zonal statistic result\n",
    "scale = scale.getInfo()\n",
    "for i in range(len(df_DEF_StatesB150)):\n",
    "    df_DEF_StatesB150[i]['loss50_ha_' + str(years[i])] = (\n",
    "        (df_DEF_StatesB150[i]['sum']*(scale**2))/10000\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# download only once the FeatureCollection of the countries in shape format\n",
    "\n",
    "# This is to get a base in shp format to join csv's fiels.\n",
    "out_dir = os.path.join('outputs_zonalStatistic/countriesSHP')\n",
    "out_countries_stats = os.path.join(out_dir, 'StatesB1_50.shp')\n",
    "    \n",
    "geemap.zonal_statistics(\n",
    "    deforestation50[0], \n",
    "    country, \n",
    "    out_countries_stats, \n",
    "    statistics_type='SUM', \n",
    "    scale=scale)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Drop unnecesary columns\n",
    "df_DEF_StatesB150_drop = []\n",
    "for i in df_DEF_StatesB150:\n",
    "    df_DEF_StatesB150_drop.append(i.drop([\n",
    "        'sum', 'system:index', 'ADM1_CODE', 'EXP2_YEAR', 'ADM2_NAME', \n",
    "        'ADM1_NAME', 'DISP_AREA', 'Shape_Leng', 'STATUS', 'STR2_YEAR',\n",
    "        'ADM0_NAME', 'ADM0_CODE', 'Shape_Area', 'areaSqKm'\n",
    "    ], axis=1))\n",
    "    \n",
    "# -----------------------------------------------------------------------------   \n",
    "gpd_StatesB1_50 = gpd.read_file(\n",
    "    r'outputs_zonalStatistic/countriesSHP/StatesB1_50.shp')\n",
    "\n",
    "gpd_StatesB1_50 = gpd_StatesB1_50.drop(['sum'], axis=1)\n",
    "\n",
    "gpd_StatesB1_50 = gpd_StatesB1_50.set_index('ADM2_CODE')\n",
    "\n",
    "for i in df_DEF_StatesB150_drop:\n",
    "    gpd_StatesB1_50 = pd.concat([gpd_StatesB1_50, i.set_index('ADM2_CODE')],\n",
    "                                axis=1)\n",
    "# -----------------------------------------------------------------------------\n",
    "out_dir = os.path.join('outputs_zonalStatistic')\n",
    "out_countries_stats = os.path.join(out_dir,\n",
    "                                   'treecover2000_50_StatesB1.csv')\n",
    "    \n",
    "geemap.zonal_statistics(\n",
    "    treecover50, \n",
    "    country, \n",
    "    out_countries_stats, \n",
    "    statistics_type='SUM',\n",
    "    scale=scale)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# concat with treecover 50%\n",
    "df_treecover2000_StatesB150 = pd.read_csv(\n",
    "    r'outputs_zonalStatistic/treecover2000_50_StatesB1.csv')\n",
    "\n",
    "df_treecover2000_StatesB150['forest_50'] = (\n",
    "    (df_treecover2000_StatesB150['sum']*(scale**2))/10000)\n",
    "\n",
    "df_treecover2000_StatesB150 = df_treecover2000_StatesB150.drop(\n",
    "    ['sum', 'system:index', 'ADM1_CODE', 'ADM2_NAME', 'EXP2_YEAR',\n",
    "     'ADM1_NAME', 'DISP_AREA', 'Shape_Leng', 'STATUS', 'STR2_YEAR',\n",
    "     'ADM0_NAME', 'ADM0_CODE', 'Shape_Area','areaSqKm'], \n",
    "    axis=1)\n",
    "\n",
    "gpd_StatesB1_50 = pd.concat([\n",
    "    gpd_StatesB1_50, \n",
    "    df_treecover2000_StatesB150.set_index('ADM2_CODE')], axis=1)\n",
    "\n",
    "gpd_StatesB1_50 = gpd_StatesB1_50.drop(\n",
    "    ['loss50_ha_2001', 'loss50_ha_2002', \n",
    "    'loss50_ha_2003', 'loss50_ha_2004',\n",
    "    'loss50_ha_2005', 'loss50_ha_2006',\n",
    "    'loss50_ha_2007', 'loss50_ha_2008'], axis=1)\n",
    "\n",
    "\n",
    "gpd_StatesB1_50.to_file('gpkg/V1/thresholds.gpkg', layer='StatesB1_50',\n",
    "                        driver=\"GPKG\")\n",
    "\n",
    "gpd_StatesB1_50[['areaSqKm','forest_50']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb9243-a096-41c6-b8ae-bc22f7f8c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = StatesB2\n",
    "for i in range(len(consecutive_numbers)):\n",
    "    out_dir = os.path.join('outputs_zonalStatistic')\n",
    "    out_countries_stats = os.path.join(\n",
    "        out_dir, lossyear50[i] + 'StatesB2'+'.csv')\n",
    "    \n",
    "    geemap.zonal_statistics(\n",
    "        deforestation50[i],\n",
    "        country,\n",
    "        out_countries_stats,\n",
    "        statistics_type='SUM',\n",
    "        scale=scale\n",
    "    )\n",
    "#-------------------------------------------------------------------\n",
    "# Load csv layers generated in the previous step\n",
    "\n",
    "filename = r'outputs_zonalStatistic/'\n",
    "#nn = np.arange(2001,2019,1)\n",
    "filenames = ['lossyear50_' + str(f) + 'StatesB2'+'.csv' for f in years]\n",
    "\n",
    "df_DEF_StatesB250 = [pd.read_csv(filename + j) for j in filenames]\n",
    "\n",
    "# Calculate area in ha from result of the zonal statistic\n",
    "scale = scale.getInfo()\n",
    "for i in range(len(df_DEF_StatesB250)):\n",
    "    df_DEF_StatesB250[i]['loss50_ha_' + str(years[i])] = (\n",
    "        (df_DEF_StatesB250[i]['sum']*900)/10000\n",
    "    )\n",
    "    \n",
    "# download for only one time the FeatureCollection of the countries in\n",
    "# shape format\n",
    "\n",
    "# This is to get a base in shp format to join csv's fiels.\n",
    "out_dir = os.path.join('outputs_zonalStatistic/countriesSHP')\n",
    "out_countries_stats = os.path.join(out_dir, 'StatesB2_50.shp')\n",
    "    \n",
    "geemap.zonal_statistics(\n",
    "    deforestation50[0], \n",
    "    country, \n",
    "    out_countries_stats, \n",
    "    statistics_type='SUM', \n",
    "    scale=scale)\n",
    "    \n",
    "# Drop unnecesary columns\n",
    "df_DEF_StatesB250_drop = []\n",
    "for i in df_DEF_StatesB250:\n",
    "    df_DEF_StatesB250_drop.append(i.drop([\n",
    "        'sum', 'system:index', 'ADM1_CODE', 'EXP2_YEAR', 'ADM2_NAME', \n",
    "        'ADM1_NAME', 'DISP_AREA', 'Shape_Leng', 'STATUS', 'STR2_YEAR',\n",
    "        'ADM0_NAME', 'ADM0_CODE', 'Shape_Area', 'areaSqKm'\n",
    "    ], axis=1))\n",
    "    \n",
    "    \n",
    "gpd_StatesB2_50 = gpd.read_file(\n",
    "    r'outputs_zonalStatistic/countriesSHP/StatesB2_50.shp')\n",
    "gpd_StatesB2_50 = gpd_StatesB2_50.drop(['sum'], axis=1)\n",
    "\n",
    "gpd_StatesB2_50 = gpd_StatesB2_50.set_index('ADM2_CODE')\n",
    "\n",
    "for i in df_DEF_StatesB250_drop:\n",
    "    gpd_StatesB2_50 = pd.concat([gpd_StatesB2_50, i.set_index('ADM2_CODE')], \n",
    "                                axis=1)\n",
    "# -----------------------------------------------------------------------------\n",
    "out_dir = os.path.join('outputs_zonalStatistic')\n",
    "out_countries_stats = os.path.join(out_dir,\n",
    "                                   'treecover2000_50_StatesB2.csv')\n",
    "    \n",
    "geemap.zonal_statistics(\n",
    "    treecover50, \n",
    "    StatesB2, \n",
    "    out_countries_stats, \n",
    "    statistics_type='SUM',\n",
    "    scale=scale)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# concat with treecover 50%\n",
    "df_treecover2000_StatesB250 = pd.read_csv(\n",
    "    r'outputs_zonalStatistic/treecover2000_50_StatesB2.csv')\n",
    "\n",
    "df_treecover2000_StatesB250['forest_50'] = (\n",
    "    df_treecover2000_StatesB250['sum']*(scale**2))/10000\n",
    "\n",
    "df_treecover2000_StatesB250 = df_treecover2000_StatesB250.drop([\n",
    "    'sum', 'system:index', 'ADM1_CODE', 'ADM2_NAME', 'EXP2_YEAR',\n",
    "    'ADM1_NAME', 'DISP_AREA', 'Shape_Leng', 'STATUS', 'STR2_YEAR',\n",
    "    'ADM0_NAME', 'ADM0_CODE', 'Shape_Area'], axis=1)\n",
    "\n",
    "gpd_StatesB2_50 = pd.concat([\n",
    "    gpd_StatesB2_50, df_treecover2000_StatesB250.set_index('ADM2_CODE')],\n",
    "    axis=1)\n",
    "\n",
    "gpd_StatesB2_50 = gpd_StatesB2_50.drop([\n",
    "    'loss50_ha_2001', 'loss50_ha_2002', \n",
    "    'loss50_ha_2003', 'loss50_ha_2004',\n",
    "    'loss50_ha_2005', 'loss50_ha_2006',\n",
    "    'loss50_ha_2007', 'loss50_ha_2008'\n",
    "], axis=1)\n",
    "\n",
    "\n",
    "gpd_StatesB2_50.to_file('gpkg/V1/thresholds.gpkg', layer='StatesB2_50',\n",
    "                        driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7133eb58-5524-4a5d-b190-3216a6d45897",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycodestyle_off"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
